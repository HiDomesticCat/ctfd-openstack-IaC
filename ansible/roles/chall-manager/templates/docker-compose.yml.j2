# chall-manager docker-compose（由 Ansible 渲染）
# 參考文件：https://github.com/ctfer-io/chall-manager

services:
  # ── 本機 OCI Registry ──────────────────────────────────────
  # 存放 scenario OCI artifact（zip），不需要 Docker Hub 帳號
  registry:
    image: registry:2
    restart: unless-stopped
    ports:
      - "{{ registry_port }}:5000"
    volumes:
      - registry-data:/var/lib/registry

  # ── chall-manager 後端微服務 ───────────────────────────────
  chall-manager:
    image: ctferio/chall-manager:{{ chall_manager_version }}
    restart: unless-stopped
    command:
      - /chall-manager
      - --oci.insecure
    ports:
      - "127.0.0.1:{{ chall_manager_port }}:8080"
    expose:
      - "8080"
    environment:
      # 分散式鎖後端
      CM_LOCK: etcd
      CM_LOCK_ETCD_ENDPOINTS: "etcd:2379"

      # chall-manager 自己的 stack state 目錄（持久化，重啟後不消失）
      # 注意：這不是 /scenarios（scenario 程式碼），而是 chall-manager 內部狀態
      CM_DIRECTORY: /chall-manager-data

      # OpenStack 憑證（由 Pulumi scenario 程式繼承使用）
      OS_AUTH_URL: "{{ openstack_auth_url }}"
      OS_PROJECT_NAME: "{{ openstack_project_name }}"
      OS_USERNAME: "{{ openstack_username }}"
      OS_PASSWORD: "{{ openstack_password }}"
      OS_REGION_NAME: "{{ openstack_region }}"
      OS_USER_DOMAIN_NAME: "{{ openstack_user_domain }}"
      OS_PROJECT_DOMAIN_NAME: "{{ openstack_project_domain }}"
      OS_IDENTITY_API_VERSION: "3"

      # ── 題目設定（Pulumi scenario 從環境變數讀取）─────────
      # chall-manager 容器啟動的 Pulumi 子程序會繼承以下 env vars

      # openstack-vm scenario 設定
      CHALLENGE_IMAGE_ID: "{{ challenge_image_id }}"
      CHALLENGE_NETWORK_ID: "{{ challenge_network_id }}"
      CHALLENGE_FLAVOR: "{{ challenge_flavor | default('general.small') }}"
      CHALLENGE_PORT: "{{ challenge_port | default('8080') }}"
      CHALLENGE_FIP_POOL: "{{ challenge_fip_pool | default('public') }}"
      CHALLENGE_BASE_FLAG: "{{ challenge_base_flag | default('replace_me') }}"
      CHALLENGE_FLAG_PREFIX: "{{ challenge_flag_prefix | default('CTF') }}"

      # k8s-pod scenario 設定（Chell Shell Challenge 後端）
{% if k3s_worker_ips | default([]) | length > 0 %}
      CHALLENGE_IMAGE: "{{ k3s_challenge_image | default('ubuntu:22.04') }}"
      K3S_WORKER_IPS: "{{ k3s_worker_ips | join(',') }}"
      KUBECONFIG: "{{ k3s_kubeconfig_container_path | default('/kubeconfig/k3s.yaml') }}"
{% if k3s_challenge_command | default('') != '' %}
      # 測試用：覆蓋 container 啟動指令讓容器持續運行（如 sleep,infinity）
      # 正式 challenge image 通常不需要此設定
      CHALLENGE_COMMAND: "{{ k3s_challenge_command }}"
{% endif %}
{% endif %}

      # Pulumi local backend（不需要 Pulumi Cloud 帳號）
      PULUMI_BACKEND_URL: "file:///pulumi-state"
      # ⚠️  SECURITY: 空字串 passphrase 表示 pulumi-state volume 未加密。
      # State 內含資源 UUID、floating IP 等基礎設施資訊。
      # 建議生產/競賽環境設定強密碼以加密 state（遺失密碼將無法管理已部署資源）：
      #   PULUMI_CONFIG_PASSPHRASE: "{{ vault_pulumi_passphrase | default('') }}"
      PULUMI_CONFIG_PASSPHRASE: ""

    volumes:
      # ✅ 不加 :ro：chall-manager 執行 Pulumi 時可能需要寫入暫存檔
      # Pulumi state 已獨立掛載到 pulumi-state volume
      - {{ chall_manager_scenarios_dir }}:/scenarios
      - pulumi-state:/pulumi-state
      # ✅ chall-manager 自己的 stack state（持久化 volume，重啟後保留）
      - chall-manager-data:/chall-manager-data
      # ✅ OCI cache bind mount 到 VM 實體目錄
      # 比 tmpfs 更好：允許執行（無 noexec 問題），由 Ansible file 模組清理
      - {{ chall_manager_dir }}/cache:/root/.cache/chall-manager
{% if k3s_worker_ips | default([]) | length > 0 %}
      # k3s kubeconfig（供 k8s-pod scenario 連接 k3s API）
      # 由 Ansible k3s role 從 master 取得並部署到此路徑
      - {{ chall_manager_dir }}/kubeconfig/k3s.yaml:/kubeconfig/k3s.yaml:ro
{% endif %}
    depends_on:
      etcd:
        condition: service_healthy
      registry:
        condition: service_started
    networks:
      - default
      - ctfd_internal

  # ── 定期清理過期 instances ──────────────────────────────────
  chall-manager-janitor:
    image: ctferio/chall-manager-janitor:{{ chall_manager_version }}
    restart: unless-stopped
    command:
      - /chall-manager-janitor
      - --url=chall-manager:8080
      - --ticker={{ chall_manager_janitor_ticker | default('10s') }}
    depends_on:
      - chall-manager
    networks:
      - default

  # ── 分散式鎖：etcd ─────────────────────────────────────────
  etcd:
    image: {{ etcd_image }}
    restart: unless-stopped
    # ✅ 明確指定 command，避免不同版本環境變數讀取行為不一致
    command:
      - etcd
      - --data-dir=/var/lib/etcd
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://etcd:2379
    healthcheck:
      test: ["CMD", "etcdctl", "--endpoints=http://localhost:2379", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 3
    volumes:
      # ✅ 修正：官方 etcd image 預設資料目錄是 /var/lib/etcd，不是 /var/etcd
      - etcd-data:/var/lib/etcd

volumes:
  registry-data:
  pulumi-state:
  chall-manager-data:
  etcd-data:

networks:
  default:
  ctfd_internal:
    external: true